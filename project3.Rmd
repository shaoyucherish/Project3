---
title: "Project 3"
author: "Shaoyu Wang, Aniket Walimbe"
date: "`r Sys.Date()`"
output: github_document
params: 
  channelID: "lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# The Analysis for `r params[[1]]` Data Channel

## Introduction

This [online news popularity data set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. There are 61 attributes, including 58 predictive attributes, 2 non-predictive, 1 goal field. The number of shares is our target variable, and we select predictive variables from the remaining variables based on the exploratory data analysis. The purpose of our analysis is to predict the the number of shares. During this project, we read and subset the data set at first, and split data into training set and test set, then we create some basic summary statistics and plots about the training data, at last we fit linear regression models and ensemble tree-based models and test the predictions.

**Variable information**

The target variable is the following:  
  -  `shares`: Number of shares  

The predictive variables are the following:  
  - `publish_weekday`: The article published day  
  - `n_tokens_title`: Number of words in the title  
  - `n_tokens_content`: Number of words in the content  
  - `num_self_hrefs`: Number of links to other articles published by Mashable  
  - `num_imgs`: Number of images  
  - `num_videos`: Number of videos  
  - `average_token_length`: Average length of the words in the content  
  - `num_keywords`: Number of keywords in the metadata  
  - `kw_avg_avg`: Avg. keyword (avg. shares)  
  - `self_reference_avg_sharess`: Avg. shares of referenced articles in Mashable  
  - `LDA_04`: Closeness to LDA topic 4  
  - `global_subjectivity`: Text subjectivity  
  - `global_sentiment_polarity`: Text sentiment polarity  
  - `avg_positive_polarity`: Avg. polarity of positive words  
  - `avg_negative_polarity`: Avg. polarity of negative words  

**Required packages**

We need to load the required packages:
```{r}
# Load libraries
library(readr)
library(tidyverse)
library(dplyr)
library(caret)
library(leaps)
library(ggplot2)
library(corrplot)
library(GGally)
library(randomForest)
```

## Data

Read in the data and subset the data to work on the data channel of interest. We find that there are seven similar columns for weekdays from Monday to Sunday, so we merge these columns and name the new variable as `publish_weekday` and convert it to factor. For this step, we also remove the non-predictive variables.

```{r}
#Read in the data file
newsData <- read_csv("OnlineNewsPopularity.csv")
#Select the data channel of interest
selectChannel <- paste0("data_channel_is_", params[[1]])
news <- newsData %>% 
  filter(get(selectChannel) == 1) %>% 
  select(url, starts_with("weekday_is_")) %>% 
  pivot_longer(-url) %>% 
  filter(value != 0) %>% 
  mutate(publish_weekday = substr(name, 12, 20)) %>% 
  left_join(newsData, by = "url") %>% 
#Remove non predictive variables
  select(-c(url, name, value, timedelta, starts_with("data_channel_is_"), starts_with("weekday_is_")))
#convert publish_weekday to factor
news$publish_weekday <- as.factor(news$publish_weekday)
news
```

Split the data into a training set and a test set.
```{r}
set.seed(111)
trainIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)
newsTrain <- news[trainIndex,]
newsTest <- news[-trainIndex,]
#newsTrain
```

## Summarizations

For this part, we produce some basic summary statistics and plots about the training data.

**Tables**

Firstly, here is a quick summary of all variables as shown below, so that we can know the variables roughly.
```{r}
summary(newsTrain)
```

Then we can check our response variable `shares`. The below table shows that the mean, standard deviation, median, IQR of `shares`.
```{r}
#numerical summary for the variable shares
newsTrain %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), 
            median = round(median(shares), 0), IQR = round(IQR(shares), 0))
```

We also obtain the numerical summaries on some subgroups. We choose four example subgroups: number of images, number of videos, and number of keywords, since people may concern more on these when they do searching and sharing.
```{r}
#numerical summaries on subgroups
newsTrain %>% 
  group_by(num_imgs) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), 
            median = round(median(shares), 0), IQR = round(IQR(shares), 0))

newsTrain %>% 
  group_by(num_videos) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), 
            median = round(median(shares), 0), IQR = round(IQR(shares), 0))

newsTrain %>% 
  group_by(num_keywords) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), 
            median = round(median(shares), 0), IQR = round(IQR(shares), 0))
```

Moreover, we divide the title subjectivity into 3 categories:  
  1. High: greater than 0.8  
  2. Medium: 0.4 to less than 0.8  
  3. Low: less than 0.4  
The contingency table is then shown below.
```{r}
newsTrain$subject_activity_type <- ifelse(newsTrain$title_subjectivity >= 0.8, "High", 
                                          ifelse(newsTrain$title_subjectivity >= 0.4, "Medium",
                                                 ifelse(airquality$Wind >= 0, "Low")))
table(newsTrain$subject_activity_type)
```

**Plots**

At the beginning, let's plot the correlation between the numeric variables.
```{r}
newsTrainsub <- newsTrain %>% select(-c(publish_weekday, subject_activity_type))
correlation <- cor(newsTrainsub, method = "spearman")
corrplot(correlation, tl.col = "black", tl.cex = 0.5)
```

From the correlation graph above, we can see that the following variables seem to be moderately correlated:  
  - `n_tokens_contents`, `n_unique_tokens`, `n_non_stop_words`, `n_non_stop_unique_tokens`, `num_hrefs`, `num_imgs`  
  - `kw_min_min`, `kw_max_min`, `kw_avg_min`, `kw_min_max`, `kw_max_max`, `kw_avg_max`, `kw_min_avg`, `kw_max_avg`, `kw_avg_avg`  
  - `self_reference_min_shares`, `self_reference_max_shares`, `self_reference_avg_sharess`  
  - `LDA_00`, `LDA_01`, `LDA_02`, `LDA_03`  
  - `global_sentiment_polarity`, `global_rate_positive_words`, `global_rate_negative_words`, `rate_positive_words`, `rate_negative_words`  
  - `avg_positive_polarity`, `min_positive_polarity`, `max_positive_polarity`  
  - `avg_negative_polarity`, `min_negative_polarity`, `max_negative_polarity`  
  - `title_subjectivity`, `title_sentiment_polarity`, `abs_title_subjectivity`, `abs_title_sentiment_polarity`  

For further EDA, we are going to plot graphs to see trends between different variables with respect to the number of shares.

A plot between number of shares and article published day: This plot shows the number of shares an article has based on the day that has been published.
```{r}
newsTrainday <- newsTrain %>%
  select(publish_weekday, shares) %>%
  group_by(publish_weekday) %>% 
  summarise(total_shares=sum(shares))

g <- ggplot(data = newsTrainday, aes(x=publish_weekday, y=total_shares))
g + geom_col(fill = "lightblue", color = "black") +
  labs(title = " Shares for articles published based on weekdays")
```

Let's select some variables as example to plot scatter plots.

A scatter plot with the number of shares on the y-axis and the number of words in the title on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = n_tokens_title, y = shares))
g + geom_point() +
  labs(x = "Number of words in the title", y = "Number of shares", 
       title = "Scatter Plot: Number of words in the title VS Number of shares")
```

We can inspect the trend of shares as a function of the number of words in the title. Therefore, we can see that the number of words in title has an effect on the number of shares.

A scatter plot with the number of shares on the y-axis and the number of words in the content on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = n_tokens_content, y = shares))
g + geom_point() +
  labs(x = "Number of words in the content", y = "Number of shares", 
       title = "Scatter Plot: Number of words in the content VS Number of shares") 
```

From the plot above, we can easily see that the number of shares is decreasing while the the number of words in the content is increasing. So it can be illustrated that the number of words in the content will affect the number of shares.

A scatter plot with the number of shares on the y-axis and the number of links to other articles published by Mashable on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = num_self_hrefs, y = shares))
g + geom_point() +
  labs(x = "Number of links to other articles published by Mashable", y = "Number of shares", 
       title = "Scatter Plot: Number of links to other articles published by Mashable VS Number of shares")
```

The plot above shows that as the number of links to other articles increasing, the number of shares is decreasing. So the the number of links to other articles has an infulence on the number of shares.

A scatter plot with the number of shares on the y-axis and the number of images on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = num_imgs, y = shares))
g + geom_point() +
  labs(x = "Number of images", y = "Number of shares", 
       title = "Scatter Plot: Number of images VS Number of shares")
```

The plot above shows that the number of shares decreases as the number of images increasing. Therefore, the number of images will affect the number of shares as well.

A scatter plot with the number of shares on the y-axis and the number of videos on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = num_videos, y = shares))
g + geom_point() +
  labs(x = "Number of videos", y = "Number of shares", 
       title = "Scatter Plot: Number of videos VS Number of shares") 
```

A scatter plot with the number of shares on the y-axis and the average length of words in content on the x-axis is created:
```{r}
g <- ggplot(newsTrain, aes(x = average_token_length, y = shares))
g + geom_point() + 
  labs(x = "Average token length", y = "Number of shares", 
       title = "Scatter Plot: Average token length VS Number of shares")
```

Through the plot above, we can see that the most of shares are between 4 and 6 words. The average token length will also affect the number of shares.

A scatter plot with the number of shares on the y-axis and the number of keywords in the metadata on the x-axis is created:
```{r}
g <- ggplot(newsTrain, aes(x = num_keywords, y = shares))
g + geom_point() + 
  labs(x = "Number of keywords in the metadata", y = "Number of shares", 
       title = "Scatter Plot: Number of keywords in the metadata VS Number of shares")
```

According to the plot above, we can find that as the number of keywords increasing, the number of shares is increasing. So the number of keywords in the metadata will influence the number of shares.

A scatter plot with the number of shares on the y-axis and the text subjectivity on the x-axis is created:
```{r}
g <- ggplot(data = newsTrain, aes(x = global_subjectivity, y = shares))
g + geom_point() + 
  labs(x = "Text subjectivity", y = "Number of shares", 
       title = "Scatter Plot: Text subjectivity VS Number of shares")
```

From the plot above, it presents that the most of shares are between 0.25 and 0.75 text subjectivity. So the text subjectivity will influence the number of shares as well.

A scatter plot with the number of shares on the y-axis and the title subjectivity on the x-axis is created: 
```{r}
g <- ggplot(data = newsTrain, aes(x = title_subjectivity, y = shares))
g + geom_point() + 
  labs(x = "Title subjectivity", y = "Number of shares", 
       title = "Scatter Plot: Title subjectivity VS Number of shares")
```

The plot above shows that the title subjectivity has less effect on the number of shares.

**Select variables**

Through the analysis above, we will select predictors as follows:  
  - `publish_weekday`: The article published day  
  - `n_tokens_title`: Number of words in the title  
  - `n_tokens_content`: Number of words in the content  
  - `num_self_hrefs`: Number of links to other articles published by Mashable  
  - `num_imgs`: Number of images  
  - `num_videos`: Number of videos  
  - `average_token_length`: Average length of the words in the content  
  - `num_keywords`: Number of keywords in the metadata  
  - `kw_avg_avg`: Avg. keyword (avg. shares)  
  - `self_reference_avg_sharess`: Avg. shares of referenced articles in Mashable  
  - `LDA_04`: Closeness to LDA topic 4  
  - `global_subjectivity`: Text subjectivity  
  - `global_sentiment_polarity`: Text sentiment polarity  
  - `avg_positive_polarity`: Avg. polarity of positive words  
  - `avg_negative_polarity`: Avg. polarity of negative words  

The target variable is `shares`.

Let's do selection for training set and test set.
```{r}
#select variables for training set and test set
set.seed(111)
Train <- newsTrain %>% 
  select(publish_weekday, n_tokens_title, n_tokens_content, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, kw_avg_avg, self_reference_avg_sharess, LDA_04, global_subjectivity, global_sentiment_polarity, avg_positive_polarity, avg_negative_polarity, shares)
Test <- newsTest %>% 
  select(publish_weekday, n_tokens_title, n_tokens_content, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, kw_avg_avg, self_reference_avg_sharess, LDA_04, global_subjectivity, global_sentiment_polarity, avg_positive_polarity, avg_negative_polarity, shares)
```

## Model

**Linear Regression Model**

First, we fit a forward stepwise linear regression model for the training dataset. The data is centered and scaled and number of shares is the response variable.
```{r}
#forward stepwise
set.seed(111)
fwFit <- train(shares ~ ., data = Train,
               method = "leapForward",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5))
#summary(fwFit)
fwFit
```

We also fit a backward stepwise linear regression model for the training dataset. The data is centered and scaled and number of shares is the response variable.
```{r}
#backward stepwise
set.seed(111)
bwFit <- train(shares ~ ., data = Train,
               method = "leapBackward",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5))
#summary(bwFit)
bwFit
```

Then we fit a linear regression model with all predictors.
```{r}
#with all predictors
set.seed(111)
lrFit <- train(shares ~ ., data = Train,
               method = "lm",
               trControl = trainControl(method = "cv", number = 5))
lrFit
```

**Random Forest Model**

Next, we have fitted a random forest model which is an example of an ensemble based-tree model. Instead of traditional decision trees, a random forest tree will take a random subset of the predictors for each tree fit and calculate the average of results.
```{r}
set.seed(111)
randomFit <- train(shares ~ ., 
                   data = Train, 
                   method = "rf",
                   preProcess = c("center","scale"),
                   trControl = trainControl(method = "cv", number = 5),
                   tuneGrid = data.frame(mtry = ncol(Train)/3))
randomFit
```

**Boosted Tree Model**

Moreover, we have fitted a boosted tree model which is another ensemble based-tree model. Boosted tree models are combination of two techniques: decision tree algorithms and boosting methods. It repeatedly fits many decision trees to improve the accuracy of the model.
```{r}
set.seed(111)
boostedFit <- train(shares ~ ., 
                    data = Train, 
                    method = "gbm", 
                    preProcess = c("center", "scale"),
                    trControl = trainControl(method = "cv", number = 5),
                    tuneGrid = expand.grid(n.trees = c(25,50,100,150,200), 
                                           interaction.depth = c(1,2,3,4), 
                                           shrinkage = 0.1, 
                                           n.minobsinnode = 10),
                    verbose = FALSE)
boostedFit
```

## Comparison

All the models are compared by RMSE on the test set.
```{r}
#fit a linear regression model
fw_mod <- postResample(predict(fwFit, newdata = Test), obs = Test$shares)
bw_mod <- postResample(predict(bwFit, newdata = Test), obs = Test$shares)
lr_mod <- postResample(predict(lrFit, newdata = Test), obs = Test$shares)
#random forest
random_mod <- postResample(predict(randomFit, newdata = Test), obs = Test$shares)
#boosted tree
boosted_mod <- postResample(predict(boostedFit, newdata = Test), obs = Test$shares)
#compare all models
tibble(model = c("Forward",
                 "Backward",
                 "LR with all predictors",
                 "Random Forest",
                 "Boosted Tree"), 
       RMSE = c(fw_mod[1],
                bw_mod[1],
                lr_mod[1],
                random_mod[1],
                boosted_mod[1]))

result_table
min_value <- min(result_table$RMSE)
best_model <- result_table[result_table$RMSE == min_value, "model"]

print(paste0("The best model based on the lowest RMSE value is ",as.character(best_model[1,1])," with an RMSE value of ",as.character(round(min_value,2))))
```

## Automation

For this automation part, we want to produce the similar reports for each news channels. We firstly create a set of parameters, which match with 6 channels. Then read the parameter and subset the data with the specified channel. After everything is ready, run the below chunk of code in the console, we will automatically get the reports for each news channel.

```{r, include=TRUE, eval=FALSE}
#create channel names
channelID <- data.frame("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
#create filenames
output_file <- paste0(channelID,".md")
#create a list for each channel with the channel name parameter
params = lapply(channelID, FUN = function(x){list(channelID = x)})
#put into a data frame
reports <- tibble(output_file, params)
#render code
apply(reports, MARGIN = 1,
      FUN = function(x){
        rmarkdown::render(input = "project3.Rmd",
                          output_format = "github_document",
                          output_file = x[[1]],
                          params = x[[2]])
        })
```


