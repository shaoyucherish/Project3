---
title: "Project 3"
author: "Shaoyu Wang,Aniket Walimbe"
date: "2022-11-09"
output: 
  github_document
params:
    columnName: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Required Packages

The following packages are required for this project:
```{r}
# Load libraries
library(readr)
library(tidyverse)
library(dplyr)
library(caret)
library(ggplot2)
library(corrplot)
library(GGally)
library(randomForest)
```

# Introduction

This [online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) summarizes a heterogeneous set of features about articles published by Mashable in a period of two years.


# Data

Use a relative path to import the data and subset the data to work on the data channel of interest.

```{r}
#Read in the data file
newsData <- read_csv("OnlineNewsPopularity.csv",show_col_types = FALSE)
#Choose the data channel of interest
if (params$columnName != "") {
  paramColumnNameType <- params$columnName
} else {
  paramColumnNameType <- "lifestyle"
}
columnName <- paste("data_channel_is_", paramColumnNameType, sep = "")
#Merge the weekdays columns channels as one single column.
news <- newsData %>% 
  filter(.data[[columnName]] == 1) %>% 
  select(url, starts_with("weekday_is_")) %>% 
  pivot_longer(-url) %>% 
  filter(value != 0) %>% 
  mutate(publish_weekday = substr(name, 12, 20)) %>% 
  left_join(newsData, by = "url") %>% 
#Remove non predictive variables
  select(-c(url, name, value, timedelta, starts_with("data_channel_is_"), starts_with("weekday_is_")))
news$publish_weekday <- as.factor(news$publish_weekday)
news
```

Split the data into a training set and a test set.
```{r}
set.seed(111)
trainIndex <- createDataPartition(news$shares, p = 0.7, list = FALSE)
newsTrain <- news[trainIndex,]
newsTest <- news[-trainIndex,]
newsTrain
```

# Summarizations

Some basic summary statistics and plots about the training data.
```{r}
summary(newsTrain)
```

```{r}
#numerical summary for our Y variable shares
summary(newsTrain$shares)
```

```{r}
#Numerical summaries
newsTrain %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), median = round(median(shares), 0), IQR = round(IQR(shares), 0))
newsTrain %>% 
  group_by(publish_weekday) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), median = round(median(shares), 0), IQR = round(IQR(shares), 0))
newsTrain %>% 
  group_by(num_imgs) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), median = round(median(shares), 0), IQR = round(IQR(shares), 0))
newsTrain %>% 
  group_by(num_videos) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), median = round(median(shares), 0), IQR = round(IQR(shares), 0))
newsTrain %>% 
  group_by(num_keywords) %>% 
  summarise(mean = round(mean(shares), 0), sd = round(sd(shares), 0), median = round(median(shares), 0), IQR = round(IQR(shares), 0))
```

# Contingency tables
Contingency tables : Here, the title subjectivity is divided into 3 categories : high, medium and low based on the values. If the value is greater than 0.8, it is high, greater than 0.4 and less than 0.8 is medium and remaining is low. The contingency table is then shown below. 

```{r}
newsTrain$subject_activity_type <- ifelse(newsTrain$title_subjectivity >= 0.8, "High", ifelse(newsTrain$title_subjectivity >= 0.4, "Medium",
ifelse(airquality$Wind >= 0, "Low", "None")))
table(newsTrain$subject_activity_type)
```


```{r}
table(newsTrain$publish_weekday)
```

```{r}
g <- ggplot(newsTrain, aes(x = n_tokens_title))
g + geom_histogram(fill = "lightblue", binwidth = 1) + 
  labs()
```

```{r}
g <- ggplot(newsTrain, aes(x = n_tokens_content))
g + geom_histogram(fill = "lightblue") + 
  labs()
```

```{r}
g <- ggplot(newsTrain, aes(x = global_subjectivity))
g + geom_histogram(fill = "lightblue") + 
  labs()
```

```{r}
g <- ggplot(newsTrain, aes(x = global_sentiment_polarity))
g + geom_histogram(fill = "lightblue") + 
  labs()
```

```{r}
g <- ggplot(newsTrain, aes(x = rate_positive_words, y = shares))
g + geom_point() + 
  labs()
```

```{r}
g <- ggplot(newsTrain, aes(x = average_token_length, y = shares))
g + geom_point() + 
  labs()
```

Plot between title subjectivity and number of shares: We can inspect the trend of the shares as a function of title subjectivity. 
```{r}
g <- ggplot(data = newsTrain, aes(x = title_subjectivity, y = shares))
g + geom_point() + 
  labs(x = "Title subjectivity" , y = "Number of shares", title = "Scatter Plot : Title Subjectivity Vs Number of Shares") 

```

Plot between number of shares and article published day: This plot shows the number of shares an article has based on the day it has been published.
```{r}
newsTrainday <- newsTrain %>%
  select(publish_weekday, shares) %>%
  group_by(publish_weekday) %>% 
  summarise(total_shares=sum(shares))


g <- ggplot(data = newsTrainday, aes(x=publish_weekday, y=total_shares))
g + geom_col(fill = "lightblue")+
  labs(title = " Shares for articles published based on days")
```

Plot between number of images and number of shares: 
```{r}
g <- ggplot(data = newsTrain, aes(x = num_imgs, y = shares))
g + geom_point(color = "blue") +
  labs(x = "Number of Images" , y = "Number of shares", title = "Scatter Plot : Number of Images Vs Number of Shares") 
```

Plotting the correlation between numeric variables.
```{r}
correlation <- cor(newsTrain %>% select(-publish_weekday), method = "spearman")
corrplot(correlation, type = "upper", tl.pos = "lt", tl.col = "black", tl.cex = 0.5, mar = c(2, 0, 1, 0)) 
corrplot(correlation, type = "lower", add = TRUE, diag = FALSE, tl.pos = "n", number.cex = 0.5)
```

Select predictors: publish_weekday, n_tokens_title, n_tokens_content, num_self_hrefs, num_videos, average_token_length, num_keywords, kw_avg_avg, self_reference_avg_sharess, LDA_04, global_subjectivity, global_sentiment_polarity, avg_positive_polarity, avg_negative_polarity, title_subjectivity, shares
```{r}
set.seed(111)
Train <- newsTrain %>% 
  select(publish_weekday, n_tokens_title, n_tokens_content, num_self_hrefs, num_videos, average_token_length, num_keywords, kw_avg_avg, self_reference_avg_sharess, LDA_04, global_subjectivity, global_sentiment_polarity, avg_positive_polarity, avg_negative_polarity, title_subjectivity, shares)
Test <- newsTest %>% 
  select(publish_weekday, n_tokens_title, n_tokens_content, num_self_hrefs, num_videos, average_token_length, num_keywords, kw_avg_avg, self_reference_avg_sharess, LDA_04, global_subjectivity, global_sentiment_polarity, avg_positive_polarity, avg_negative_polarity, title_subjectivity, shares)
```

# Model

## Linear Regression Model
```{r}
set.seed(111)
lrFit <- train(shares ~ ., data = Train,
               method = "lm",
               trControl = trainControl(method = "cv", number = 5))
#lrFit
predict_lrFit <- predict(lrFit, newdata = Test)
postResample(predict_lrFit, obs = Test$shares)
```

## Random Forest Model
```{r}
set.seed(111)
rfFit <- train(shares ~ ., 
                         data = Train, 
                         method = "rf",
                         preProcess = c("center","scale"),
                         trControl = trainControl(method = "cv", number = 5),
                         tuneGrid = data.frame(mtry = ncol(Train)/3))
#rfFit
predict_rfFit <- predict(rfFit, newdata = Test)
postResample(predict_rfFit, obs = Test$shares)
```


## Boosted Tree Model


```{r}

## Boosted Tree Model

set.seed(111)
boostedfit <- train(shares ~ ., 
           data = Train, 
           method = "gbm", 
           preProcess = c("center", "scale"),
           trControl = trainControl(method = "cv", number = 5),
           tuneGrid = expand.grid(n.trees = c(25,50,100,150,200), 
                      interaction.depth = c(1,2,3,4), 
                      shrinkage = 0.1, 
                      n.minobsinnode = 10)
            )
predict_boostedfit <- predict(boostedfit, newdata = Test)
postResample(predict_boostedfit, obs = Test$shares)
```

















